#include <assert.h>
#include <math.h>
#include <stdbool.h>
#include <stdlib.h>

#include "tensor.h"
#include "utils.h"

Tensor tensor_empty(size_t length) {
    Tensor tensor;
    tensor.data = (double *)malloc((length > 0 ? length : 1) * sizeof(double));
    assert(tensor.data != NULL && "Memory allocation failed");

    tensor.length = length;
    tensor.shape[0] = length;

    for (size_t i = 0; i < length; ++i) {
        tensor.data[i] = 0.0f;
    }

    return tensor;
}

Tensor tensor_zeros(size_t length) { return tensor_empty(length); }

Tensor tensor_ones(size_t length) {
    Tensor ones = tensor_empty(length);

    for (size_t i = 0; i < length; ++i) {
        ones.data[i] = 1.0f;
    }
    return ones;
}

Tensor tensor_from(size_t length, double value) {
    Tensor from = tensor_empty(length);

    for (size_t i = 0; i < length; ++i) {
        from.data[i] = value;
    }
    return from;
}

// Stub generated by an LLM, but then perfected.
// I couldn't be asked to reason about this - too primitive of a functionality
// to matter
void tensor_print(Tensor tensor, int *indices, int depth, char *prefix) {
    int shape_len = (int)tensor_shape_len(tensor);
    if (depth == shape_len - 1) {
        printf("%s[", prefix);
        for (size_t i = 0; i < tensor.shape[depth]; i++) {
            indices[depth] = i;
            int index = 0;
            for (int dim_idx = 0; dim_idx < shape_len; dim_idx++) {
                index = index * tensor.shape[dim_idx] + indices[dim_idx];
            }
            printf("%f", tensor.data[index]);
            if (i < tensor.shape[depth] - 1) {
                printf(", ");
            }
        }
        printf("]");
        return;
    }

    printf("%s[\n", prefix);
    for (size_t i = 0; i < tensor.shape[depth]; i++) {
        indices[depth] = i;
        for (int j = 0; j < depth + 1; j++) {
            printf("\t");
        }
        tensor_print(tensor, indices, depth + 1, prefix);
        if (i < tensor.shape[depth] - 1) {
            printf(",\n");
        }
    }
    printf("\n");
    for (int j = 0; j < depth; j++) {
        printf("\t");
    }
    printf("%s]", prefix);
}

Tensor _tensor_rand(size_t shape[], size_t shape_len) {
    size_t length = 0;
    for (size_t i = 0; i < shape_len; i++) {
        if (length == 0)
            length = 1;
        length *= shape[i];
    }
    assert(length && "Cannot initialize a zero or negative-shaped tensor!");
    Tensor rand_tensor = tensor_zeros(length);
    for (size_t i = 0; i < length; ++i) {
        rand_tensor.data[i] = randf64();
    }
    tensor_view_from_shape(rand_tensor, shape, shape_len);
    return rand_tensor;
}

Tensor tensor_add(Tensor a, Tensor b) {
    if (b.length == 1)
        return tensor_scalar_sum(a, b);
    if (a.length == 1)
        return tensor_scalar_sum(b, a);
    assert(a.length == b.length && "Tensors need to have the same length!");
    size_t length = a.length;
    Tensor result = tensor_zeros(a.length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = a.data[i] + b.data[i];
    }
    return result;
}

Tensor tensor_sub(Tensor a, Tensor b) {
    if (b.length == 1)
        return tensor_scalar_diff(a, b);
    if (a.length == 1)
        return tensor_scalar_diff(b, a);
    assert(a.length == b.length && "Tensors need to have the same length!");
    size_t length = a.length;
    Tensor result = tensor_zeros(a.length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = a.data[i] - b.data[i];
    }
    return result;
}

Tensor tensor_mul(Tensor a, Tensor b) {
    if (b.length == 1)
        return tensor_scalar_mul(a, b);
    if (a.length == 1)
        return tensor_scalar_mul(b, a);
    assert(a.length == b.length && "Tensors need to have the same length!");
    size_t length = a.length;
    Tensor result = tensor_zeros(a.length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = a.data[i] * b.data[i];
    }
    return result;
}

Tensor tensor_div(Tensor a, Tensor b) {
    if (b.length == 1)
        return tensor_scalar_div(a, b);
    assert(a.length == b.length && "Tensors need to have the same length!");
    size_t length = a.length;
    Tensor result = tensor_zeros(a.length);
    for (size_t i = 0; i < length; ++i) {
        assert(b.data[i] > 0 && "Division by zero!");
        result.data[i] = a.data[i] / b.data[i];
    }
    return result;
}

Tensor tensor_dot(Tensor a, Tensor b) {
    size_t a_shape_len = tensor_shape_len(a);
    size_t b_shape_len = tensor_shape_len(b);
    if (a_shape_len == 1 && b_shape_len == 1)
        return tensor_mul(a, b);

    assert(a.shape[a_shape_len - 1] == b.shape[b_shape_len - 2] &&
           "The inner sizes must be equal for the dot product to be possible");

    size_t n = a.shape[a_shape_len - 1];

    size_t rows = 1;
    for (size_t i = 0; i < a_shape_len - 1; ++i) {
        rows *= a.shape[i];
    }
    size_t cols = b.shape[a_shape_len - 1];

    Tensor result = tensor_zeros(rows * cols);
    for (size_t i = 0; i < rows; ++i) {
        for (size_t j = 0; j < cols; ++j) {
            for (size_t k = 0; k < n; ++k) {
                result.data[i * cols + j] +=
                    a.data[i * n + k] * b.data[k * cols + j];
            }
        }
    }
    return result;
}

Tensor tensor_sum(Tensor t) {
    size_t length = t.length;
    Tensor result = tensor_empty(1);
    for (size_t i = 0; i < length; ++i) {
        result.data[0] += t.data[i];
    }
    return result;
}

Tensor tensor_scalar_sum(Tensor t, Tensor scalar) {
    assert(scalar.length == 1 && "This is not a scalar value");
    size_t length = t.length;
    Tensor result = tensor_zeros(t.length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = scalar.data[0] + t.data[i];
    }
    return result;
}

Tensor tensor_scalar_diff(Tensor t, Tensor scalar) {
    assert(scalar.length == 1 && "This is not a scalar value");
    size_t length = t.length;
    Tensor result = tensor_zeros(t.length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = t.data[i] - scalar.data[0];
    }
    return result;
}

Tensor tensor_scalar_mul(Tensor t, Tensor scalar) {
    assert(scalar.length == 1 && "This is not a scalar value");
    size_t length = t.length;
    Tensor result = tensor_ones(t.length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = scalar.data[0] * t.data[i];
    }
    return result;
}

Tensor tensor_scalar_div(Tensor t, Tensor scalar) {
    assert(scalar.length == 1 && "This is not a scalar value");
    assert(scalar.data[0] > 0 && "Division by zero");
    size_t length = t.length;
    Tensor result = tensor_ones(t.length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = t.data[i] / scalar.data[0];
    }
    return result;
}

Tensor tensor_scalar_pow(Tensor base, Tensor pow) {
    assert(pow.length == 1 && "This is not a scalar value");
    size_t length = base.length;
    Tensor result = tensor_ones(base.length);
    double exponent = pow.data[0];
    if (exponent > 0) {
        for (size_t i = 0; i < length; ++i) {
            for (size_t j = 0; j < exponent; ++j) {
                result.data[i] *= base.data[i];
            }
        }
    } else if (exponent < 0) {
        for (size_t i = 0; i < length; ++i) {
            for (size_t j = 0; j < exponent * (-1); ++j) {
                result.data[i] *= base.data[i];
            }
            result.data[i] = 1 / result.data[i];
        }
    } else {
        for (size_t i = 0; i < length; ++i) {
            result.data[i] = 1;
        }
    }
    return result;
}

Tensor tensor_pow(Tensor tensor, Tensor pow) {
    assert(tensor.length == pow.length &&
           "Tensors need to have the same length!");
    size_t length = tensor.length;
    Tensor result = tensor_ones(length);
    for (size_t i = 0; i < length; ++i) {
        for (size_t times = 0; times < pow.data[i]; ++times) {
            result.data[i] *= tensor.data[i];
        }
    }
    return result;
}

Tensor tensor_scalar_sq(Tensor tensor) {
    size_t length = tensor.length;
    Tensor result = tensor_ones(length);
    for (size_t i = 0; i < length; ++i) {
        result.data[i] = tensor.data[i] * tensor.data[i];
    }
    return result;
}

Tensor tensor_scalar_inverted(Tensor tensor) {
    return tensor_scalar_pow(tensor, tensor_new_scalar(-1));
}

Tensor tensor_natural_log(Tensor t) {
    Tensor result = tensor_zeros(t.length);
    double eps = 1e-10;
    for (size_t i = 0; i < t.length; ++i) {
        double x = t.data[i];
        result.data[i] = (double)log(x > eps ? x : eps);
    }
    return result;
}

Tensor tensor_new_scalar(double value) {
    Tensor scalar = tensor_empty(1);
    scalar.data[0] = value;
    return scalar;
}

void tensor_reset_shape(Tensor *t) {
    t->shape[0] = t->length;
    for (size_t i = 1; i < MAX_DIMS; ++i) {
        t->shape[i] = 0;
    }
}

Tensor tensor_copy(Tensor tensor) {
    Tensor new_tensor = tensor_zeros(tensor.length);
    for (size_t i = 0; i < tensor.length; ++i) {
        new_tensor.data[i] = tensor.data[i];
    }
    for (size_t i = 0; i < tensor_shape_len(tensor); ++i) {
        new_tensor.shape[i] = tensor.shape[i];
    }
    return new_tensor;
}

void tensor_free(Tensor tensor) {
    tensor.length = 0;
    tensor.shape[0] = 0;
    free(tensor.data);
}

size_t tensor_shape_len(Tensor tensor) {
    assert(tensor.shape[0] != 0 && "0-dimensional tensors are not a thing...");
    size_t shape_len = 0;
    size_t shape_idx = 0;
    while (tensor.shape[shape_idx++] != 0) {
        shape_len++;
    }
    return shape_len;
}
